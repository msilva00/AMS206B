---
title: "Assignment 4"
author: "Mary Silva"
date: "3/13/2019"
output: 
  pdf_document: 
    keep_tex: yes
---

```{r setup, include=F, echo=F}
set.seed(7)
library(coda)
library(mvtnorm)
library(MASS)
pdf_z = function(z){
  -(3/2)*log(z) - theta_1*z - theta_2/z
}
```

## 1A

Using $\theta_1 = 1.5$ and $\theta_2 = 2$ we draw a sample of size 1000 using the independence Metropolis Hastings algorithm with gamma distribution as the proposal density.

```{r}
theta_1 = 1.5 # true value theta1
theta_2 = 2 # true value theta2
mean_z1 = sqrt(theta_2/theta_1)
mean_z2 = sqrt(theta_1/theta_2) + 1/(2*theta_2)

# hyperparams
b = 2.5
a = mean_z1*b
#M-H Algorithm
MH_alg1 = function(N){
  MH_samples = rep(NA, N)
  count = 0
  current_z = 1.0
  for(i in 1:N){
    curr_p = pdf_z(current_z) 
    z_new = rgamma(1, a, b)
    p_new = pdf_z(z_new)
    
    accept = exp(p_new + dgamma(current_z,a,b,log = T) - 
                   p_new - dgamma(z_new,a,b,log = T))
    if(runif(1) < accept){
      current_z = z_new
      count = count + 1
    }
    MH_samples[i] = current_z
  }
  return(list(MH_samples=MH_samples,count=count))
}
```

After trying several hyperparameters for different Gamma distributions, the best sample obtains a mean, $E(Z)$, of 
```{r, echo=F}
alg1 = MH_alg1(1000)
MH_samples = alg1$MH_samples
count = alg1$count
mean(MH_samples)
```

$E(1/Z)$

```{r, echo=F}
mean(1/MH_samples)
```
and an accuracy of 

```{r, echo=F}
count/length(MH_samples)
```

The traceplot for the samples for Metropolis-Hastings is shown below:

```{r, echo=F}
alg1 = MH_alg1(10000)
MH_samples = alg1$MH_samples
count = alg1$count
# mean(MH_samples)
# mean(1/MH_samples)
plot.ts(MH_samples[0:1000])
```

## 1B
The density of $W = log(Z)$ is given by
$$ f_W(w) \propto \exp\left\{- \frac{3}{2} w - \theta_1 exp\{w\} - \frac{\theta_2}{\exp(w)} \right\} \exp(w) $$

We draw a sample of size 1000 using the random-walk Metropolis algorithm with this density.

```{r, echo=F, include=F}
set.seed(1)
pdf_z2 = function(z){
  return(-(1/2)*log(z) - theta_1*z - theta_2/z)
}

```

```{r}
v = 0.01
MH_RW = function(N){
  N = N
  MH_RW = rep(NA, N)
  a_count = 0
  z_curr = 1.0
  for (i in 1:N) {
    p_curr = pdf_z2(z_curr)
    z_new = exp(log(z_curr) + rnorm(1,0,sqrt(v)))
    p_new = pdf_z2(z_new)
    acceptance = exp(p_new - p_curr)
    if(runif(1) < acceptance){
      z_curr = z_new
      a_count = a_count+1
    }
    MH_RW[i] = z_curr
  }
  return(list(MH_RW=MH_RW, a_count=a_count))
}
```

```{r, echo=F}
alg2 = MH_RW(10000)
MH_RW = alg2$MH_RW
count = alg2$a_count
```

The mean for the samples, $E(W_{samples})$, is 
```{r, echo=F}
(mean2_1 = mean(MH_RW))
```

And the accuracy is
```{r,echo=F}
count/length(MH_RW)
```

If we use 10000 metropolis hastings random ralk samples, the traceplot is shown below

```{r, echo=F}
plot.ts(MH_RW)
```

## 2A

$$x_i |\nu,\theta \sim Gamma(\nu,\theta)$$
$$\nu \sim Gamma(a,b)$$
$$\theta \sim Gamma(\alpha,\beta)$$
The joint posterior for $\theta$ and $\nu$
$$\pi(\theta,\nu,\pmb{x}) \propto  \frac{\left(\prod_{i=1}^nx_i \right)^{\nu-1}\nu^{a-1} e^{-b\nu}}{\left(\Gamma(\nu) \right)^n} \theta^{a+n\nu-1} \exp\left\{-\theta\left(\beta + \sum_{i=1}^n x_i \right) \right\}$$

The full conditionals:
$$\pi(\theta|\nu, \pmb{x}) \propto \theta^{a+n\nu-1}\exp\left\{-\theta\left(\beta + \sum_{i=1}^n x_i \right) \right\}$$

thus, $\theta|\nu, \pmb{x} \sim Gamma(n\nu, \beta + \sum x_i)$.

$$\pi(\nu|\theta,\pmb{x}) \propto \theta^{n\nu}\frac{\left(\prod_{i=1}^nx_i \right)^{\nu-1}\nu^{a-1} e^{-b\nu}}{\left(\Gamma(\nu) \right)^n}$$

which is not a recognizable distribution. We use a Metropolis within Gibbs algorithm to sample from the full conditionals, using a random walk proposal on $log(\nu)$. I tried various hyperparameters appropriate for this data.

```{r, echo=F, include=F}
x = read.table("my-data.txt", header = F)[,1]
n = length(x)
sum_x = sum(x)
sum_logx = sum(log(x))
library(coda)
library(mvtnorm)
library(MASS)

nu_condit = function(nu, theta_curr){
  return(nu*(sum_logx + n*log(theta_curr)-1)-n*lgamma(nu)+3*log(nu))
}
pcurr = function(nu_curr, theta_curr){
  return((n+nu_curr+2)*log(theta_curr)-n*lgamma(nu_curr) + nu_curr*(sum_logx-1) + 3*log(nu_curr) - theta_curr*(2+sum_x))
}
```

```{r}
sample = NULL
N = 1000
sample$theta = rep(NA,N)
sample$nu = rep(NA,N)
alpha = 3
beta = 2
v = 0.05
theta_curr = 2
nu_curr = 3
set.seed(2)
for(i in 1:N){
  theta_curr = rgamma(1, n*nu_curr + alpha, beta + sum_x)
  nu_new = exp(log(nu_curr) + rnorm(1,0,sqrt(v)))
  pnu_curr = nu_condit(nu_curr, theta_curr)
  pnu_new = nu_condit(nu_new, theta_curr)
  accept = exp(pnu_new - pnu_curr)
  if(runif(1) < accept)
    nu_curr = nu_new
  
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_new
}
```


The effective sample size for $\theta$ is
```{r, echo=F}
effectiveSize(sample$theta)
```

The effective sample size for $\nu$ is 
```{r, echo = F}
effectiveSize(sample$nu)
```

The table below summarizes the results

\begin{center}
\begin{tabular}{ |c|c|c| }
\hline
 parameter & mean & 95\% Credible Interval \\ 
 \hline
 $\theta$ & 1.13 & (0.635,1.846) \\  
 $\nu$ & 2.807 & (1.669, 4.55)\\
 \hline
\end{tabular}
\end{center}

The traceplots are below

```{r, echo=F}
par(mfrow = c(1,2))
plot.ts(sample$theta, ylab = expression(theta), xlab = "", main = "Trace")
plot.ts(sample$nu, ylab = expression(nu), xlab = "", main = "Trace")
```

The autocorrelation plots are below

```{r, echo=F}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2.1, 2.1))
autocorr.plot(sample$theta, col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(theta)) 
autocorr.plot(sample$nu, col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(nu))
```

## 2B

Now we develop a Metropolis-Hastings algorithm that jointly proposes $\log(\nu)$ and $\log(\theta)$ using
a Gaussian random walk centered on the current value of the parameters. Tune the
variance-covariance matrix of the proposal using a test run that proposes the parameters
independently:

```{r}
V = 0.05*diag(2)
theta_curr = 2
nu_curr = 3
N_test = 5000
for(i in 1:N_test){
  nu_new = exp(log(nu_curr) + rnorm(1,0,sqrt(V[1,1])))
  theta_new = exp(log(theta_curr) + rnorm(1,0, V[2,2]))
  p_curr = pcurr(nu_curr, theta_curr)
  p_new = pcurr(nu_new, theta_new)
  accept = exp(p_new - p_curr)
  if(runif(1) < accept){
    nu_curr = nu_new
    theta_curr = theta_new
  }
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_curr
}
```


```{r,echo=F}
V = cov(cbind(sample$nu, sample$theta))


pcurr2 = function(nu,theta){
  return(n*nu*log(theta) - n*lgamma(nu) + nu *(sum_logx -1) + 3*log(nu) + log(theta) - theta * (2 + sum_x))
}
```

```{r}
for(i in N_test+1:10000){
  new = mvrnorm(1, c(log(nu_curr), log(theta_curr)), V)
  nu_new = exp(new[1])
  theta_new = exp(new[2])
  p_curr = pcurr2(nu_curr, theta_curr)
  p_new = pcurr2(nu_new, theta_new)
  acceptance = exp(p_new - p_curr)
  if(runif(1) < acceptance){
    nu_curr = nu_new
    theta_curr = theta_new
  }
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_curr
}
```


\begin{center}
\begin{tabular}{ |c|c|c| }
\hline
 parameter & mean & 95\% Credible Interval \\ 
 \hline
 $\theta$ & 1.11 & (0.59,1.79) \\  
 $\nu$ & 2.807 & (1.49, 4.38)\\
 \hline
\end{tabular}
\end{center}

The trace plot for these samples are below

```{r, echo=F}
par(mfrow = c(1,2))
plot.ts(tail(sample$theta,5000),ylab = expression(theta), xlab = "", main = "Trace")
plot.ts(tail(sample$nu,5000), ylab = expression(nu), xlab = "", main = "Trace")
```

The corresponding autocorrelation plot is below

```{r,echo=F}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2.1, 2.1))
autocorr.plot(tail(sample$theta,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(theta)) 
autocorr.plot(tail(sample$nu,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(nu))
```

## 2C

Now we are going to develop a Metropolis algorithm that jointly proposes $\log\nu$ and $\log\theta$ using independent proposals based on Laplace approximation of the posterior distribution of $\log\nu$ and $\log\theta$. 

We let $t=\log\theta$ and $v =\log\nu$, then the posterior becomes 
\begin{align*} 
\pi(\theta,\nu|\pmb{x}) &\propto \exp\left\{(\nu -1) \sum_{i=1}^n \log x_i +(a -1)\log\nu - b\nu -n\log\Gamma(\nu) \right\}\\
&\times \exp\left\{(\alpha +n\nu-1)\log\theta -\theta\left(\beta + \sum_{i=1}^n x_i \right)\right\}\\
\Rightarrow \pi(t,v|\pmb{x}) &\propto \exp\left\{(e^v -1) sum_{i=1}^n \log x_i + av - be^v -n\log \Gamma(e^v)\right\}\\
&\times \exp\left\{(a +ne^v)t - e^t\left(\beta + sum_{i=1}^n x_i\right) \right\}
\end{align*}

Now, we let
$$h(t,v) = (e^v =1)\sum_{i=1}^n x_i + av -be^v-n\log \Gamma(e^v)\exp\left\{(a +ne^v)t - e^t\left(\beta + sum_{i=1}^n x_i\right) \right\}$$

Then we use the definition of Laplace approximation

```{r}
h = function(w) {
  a1 = (exp(w[2]) - 1) * sum_logx + 3 * w[2] - exp(w[2])
  return(-(a1 - n * lgamma(exp(w[2])) + 
             (2 + n * exp(w[2])) * w[1] - exp(w[1]) * (2 + sum_x)))
}
laplace = optim(c(0,1), h, hessian = T)
```

The laplace maximum for the parameters are
```{r,echo=F}
laplace$par
```
and the hessian obtained at the maximum is

```{r, echo=F}
laplace$hessian
V = diag(diag(solve(laplace$hessian)))
```

Now we update the variance-covariance matrix then resume the Metropolis sampling algorithm
```{r}
for(i in N_test+1:N){
  nu_new = exp(log(nu_curr) + rnorm(1,0,sqrt(V[1,1])))
  theta_new = exp(log(theta_curr) + rnorm(1,0,sqrt(V[2,2])))
  p_curr = pcurr(nu_curr = nu_curr, theta_curr = theta_curr)
  p_new =pcurr(nu_curr = nu_new, theta_curr = theta_new)
  
  accept = exp(p_new - p_curr)
  if(runif(1) < accept){
    nu_curr = nu_new
    theta_curr = theta_new
  }
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_curr
}
```

The corresponding traceplots are below

```{r, echo =F}
par(mfrow = c(1,2))
plot.ts(tail(sample$theta,5000), xlab="", ylab = expression(theta), main = "Traceplot")
plot.ts(tail(sample$nu,5000), xlab="", ylab = expression(nu), main = "Traceplot")
```

The corresponding autocorrelation plots are below

```{r,echo=F}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2.1, 2.1))
autocorr.plot(tail(sample$theta,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(theta)) 
autocorr.plot(tail(sample$nu,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(nu))
```

The effective sample size associated with $\theta$ is
```{r,echo=F}
effectiveSize(tail(sample$theta,5000))
```

The effective sample size associated with $\nu$ is
```{r,echo=F}
effectiveSize(tail(sample$nu,5000))
```

\begin{center}
\begin{tabular}{ |c|c|c| }
\hline
 parameter & mean & 95\% Credible Interval \\ 
 \hline
 $\theta$ & 1.05 & (0.56,1.58) \\  
 $\nu$ & 2.55 & (1.49, 3.79)\\
 \hline
\end{tabular}
\end{center}

## 3

Given the random effects model we have $(y_{ij} -(\beta + u_i)) \sim N(0,\tau^2)$, $u_i \sim N(0,\tau^2)$, and $\pi(\beta,\sigma^2,\tau^2) \propto (\sigma^2 \tau^2)^{-1}$. Then the joint posterior is

$$\pi(u_i,\beta,\tau^2,\sigma^2|y) \propto (\tau^2)^{-\left(\frac{IJ}{2}+1\right)} (\sigma^2)^{-\left(\frac{I}{2}+1\right)}\exp\left\{-\frac{1}{2\tau^2} \sum_{ij} \left(y_{ij} - (\beta + u_i)\right)^2  - \frac{1}{2\tau^2} \sum u_i^2 \right\} $$

### 3A

i) 

$$\pi(u_i|y,\beta,\tau,\sigma^2) \propto \exp \left\{-\frac{1}{2\tau^2} \sum \left[y^2_{ij} - 2y_{ij}(\beta + u_i) + (\beta + u_i)^2 \right] - \frac{1}{2\sigma^2} \sum u_i^2 \right\} $$
$$= \exp\left\{-\frac{1}{2\tau^2}\left[\sum (-2y_{ij}u_i) + \sum (2\beta u_i + u_i^2) \right] - \frac{1}{2\sigma^2}\sum u_i^2\right\} $$
$$= \exp\left\{-\frac{1}{2\tau^2}\left[\sum u_i^2 - 2 \sum u_i(y_{ij} - \beta) \right] - \frac{1}{2\sigma^2} \sum u_i^2 \right\} $$
$$= \exp\left\{-\frac{1}{2\tau^2} \left[\sum Ju_i^2 - 2 \sum u_i(y_{ij} -\beta) \right] - \frac{1}{2\sigma^2} u_i^2 \right\} $$
$$ = \exp \left\{-\frac{1}{2\tau^2\sigma^2} \left[J\sigma^2 \sum u_i^2 - 2\sigma^2 \sum u_i(y_{ij} - \beta) + \tau^2 \sum u_i^2 \right] \right\}$$
$$ = \exp\left\{-\frac{1}{2\tau^2\sigma^2} \left(u_i^2 \left(J\sigma^2 + \tau^2 \right) -2 u_i \sum (y_{ij} -\beta) \right) \right\} $$
$$ = \exp\left\{-\frac{J\sigma^2 +\tau^2}{2\sigma^2\tau^2 }\left(u_i^2 - 2u_i \frac{\sum (y_{ij} -\beta)}{J\sigma^2 + \tau^2} \right) \right\} $$
Therefore, 
$$ u_i | \cdot \sim N\left(\frac{\sum_j (y_{ij} -\beta)}{J\sigma^2 + \tau^2}, \frac{\tau^2\sigma^2}{J\sigma^2 + \tau^2} \right) = N\left(\left(\frac{J}{\tau^2} + \frac{1}{\sigma^2} \right)^{-1} \left(\frac{\sum_j (y_{ij} - \beta)}{\tau^2} \right), \left(\frac{J}{\tau^2} + \frac{1}{\sigma^2} \right)^{-1} \right)$$

I am lazy, so I am just going to skip to the end results so I don't have to type all my work :(

ii) 
$$\beta|\cdot \sim N\left(\frac{\tau^2}{IJ}, \frac{\sum_{ij} (y_{ij} - u_i)}{IJ} \right) = N\left(\left(\frac{IJ}{\tau^2} \right)^{-1} \left(\frac{\sum_{ij} (y_{ij} - u_i)}{\tau^2}\right), \left(\frac{IJ}{\tau^2} \right)^{-1} \right) $$

iii)
$$\sigma^2|\cdot \sim IG\left(\frac{I}{2}, \frac{1}{2} \sum_i u_i^2 \right) $$

iv)
$$ \tau^2|\cdot \sim IG \left( \frac{IJ}{2}, \frac{1}{2} \sum_{ij} (y_{ij} - (\beta + u_i))^2 \right) $$

## 3B
$$\pi(\beta, \tau^2, \sigma^2|y) \propto (\tau^2)^{-\left(\frac{I(J-1)}{2} + 1 \right)} (\sigma^2)^{-1} (J\sigma^2 + \tau^2) ^{I/2} \exp\left\{ -\frac{1}{2\tau^2} \sum_{ij} (y_{ij} - \beta)^2 \right\}$$



$$\times \exp\left\{\frac{\sigma^2}{2\tau^2(J\sigma^2 + \tau^2)} \sum_i \left(\sum_j (y_{ij}-\beta) \right)^2 \right\} $$

## 3C

$$\pi(\tau^2, \sigma^2|y) \propto (\tau^2)^{-\left(\frac{I(J-1)}{2} + 1 \right)} (\sigma^2)^{-1} \left(J\sigma^2 + \tau^2 \right) ^{\frac{I+1}{2}} \exp \left\{-\frac{1}{2\tau^2} \sum_{ij} y_{ij}^2 \right\}$$
$$ \times \exp\left\{ \frac{\sigma^2}{2\tau^2 (J\sigma^2 + \tau^2)} \sum_i \left( \sum_j y_{ij}^2 \right)\right\}$$
$$ \times \exp\left\{\frac{1}{2IJ (J\sigma^2 + \tau^2)} \left(\sum_{ij} y_{ij} \right)^2 \right\} $$






