---
title: "Assignment 4"
author: "Mary Silva"
date: "3/13/2019"
output: 
  pdf_document: 
    keep_tex: yes
---

```{r setup, include=F, echo=F}
set.seed(7)
library(coda)
library(mvtnorm)
library(MASS)
pdf_z = function(z){
  -(3/2)*log(z) - theta_1*z - theta_2/z
}
```

## 1A

Using $\theta_1 = 1.5$ and $\theta_2 = 2$ we draw a sample of size 1000 using the independence Metropolis Hastings algorithm with gamma distribution as the proposal density.

```{r}
theta_1 = 1.5 # true value theta1
theta_2 = 2 # true value theta2
mean_z1 = sqrt(theta_2/theta_1)
mean_z2 = sqrt(theta_1/theta_2) + 1/(2*theta_2)

# hyperparams
b = 2.5
a = mean_z1*b
#M-H Algorithm
MH_alg1 = function(N){
  MH_samples = rep(NA, N)
  count = 0
  current_z = 1.0
  for(i in 1:N){
    curr_p = pdf_z(current_z) 
    z_new = rgamma(1, a, b)
    p_new = pdf_z(z_new)
    
    accept = exp(p_new + dgamma(current_z,a,b,log = T) - 
                   p_new - dgamma(z_new,a,b,log = T))
    if(runif(1) < accept){
      current_z = z_new
      count = count + 1
    }
    MH_samples[i] = current_z
  }
  return(list(MH_samples=MH_samples,count=count))
}
```

After trying several hyperparameters for different Gamma distributions, the best sample obtains a mean, $E(Z)$, of 
```{r, echo=F}
alg1 = MH_alg1(1000)
MH_samples = alg1$MH_samples
count = alg1$count
mean(MH_samples)
```

$E(1/Z)$

```{r, echo=F}
mean(1/MH_samples)
```
and an accuracy of 

```{r, echo=F}
count/length(MH_samples)
```

The traceplot for the samples for Metropolis-Hastings is shown below:

```{r, echo=F}
alg1 = MH_alg1(10000)
MH_samples = alg1$MH_samples
count = alg1$count
# mean(MH_samples)
# mean(1/MH_samples)
plot.ts(MH_samples[0:1000])
```

## 1B
The density of $W = log(Z)$ is given by
$$ f_W(w) \propto \exp\left\{- \frac{3}{2} w - \theta_1 exp\{w\} - \frac{\theta_2}{\exp(w)} \right\} \exp(w) $$

We draw a sample of size 1000 using the random-walk Metropolis algorithm with this density.

```{r, echo=F, include=F}
set.seed(1)
pdf_z2 = function(z){
  return(-(1/2)*log(z) - theta_1*z - theta_2/z)
}

```

```{r}
v = 0.01
MH_RW = function(N){
  N = N
  MH_RW = rep(NA, N)
  a_count = 0
  z_curr = 1.0
  for (i in 1:N) {
    p_curr = pdf_z2(z_curr)
    z_new = exp(log(z_curr) + rnorm(1,0,sqrt(v)))
    p_new = pdf_z2(z_new)
    acceptance = exp(p_new - p_curr)
    if(runif(1) < acceptance){
      z_curr = z_new
      a_count = a_count+1
    }
    MH_RW[i] = z_curr
  }
  return(list(MH_RW=MH_RW, a_count=a_count))
}
```

```{r, echo=F}
alg2 = MH_RW(10000)
MH_RW = alg2$MH_RW
count = alg2$a_count
```

The mean for the samples, $E(W_{samples})$, is 
```{r, echo=F}
(mean2_1 = mean(MH_RW))
```

And the accuracy is
```{r,echo=F}
count/length(MH_RW)
```

If we use 10000 metropolis hastings random ralk samples, the traceplot is shown below

```{r, echo=F}
plot.ts(MH_RW)
```

## 2A

$$x_i |\nu,\theta \sim Gamma(\nu,\theta)$$
$$\nu \sim Gamma(a,b)$$
$$\theta \sim Gamma(\alpha,\beta)$$
The joint posterior for $\theta$ and $\nu$
$$\pi(\theta,\nu,\pmb{x}) \propto  \frac{\left(\prod_{i=1}^nx_i \right)^{\nu-1}\nu^{a-1} e^{-b\nu}}{\left(\Gamma(\nu) \right)^n} \theta^{a+n\nu-1} \exp\left\{-\theta\left(\beta + \sum_{i=1}^n x_i \right) \right\}$$

The full conditionals:
$$\pi(\theta|\nu, \pmb{x}) \propto \theta^{a+n\nu-1}\exp\left\{-\theta\left(\beta + \sum_{i=1}^n x_i \right) \right\}$$

thus, $\theta|\nu, \pmb{x} \sim Gamma(n\nu, \beta + \sum x_i)$.

$$\pi(\nu|\theta,\pmb{x}) \propto \theta^{n\nu}\frac{\left(\prod_{i=1}^nx_i \right)^{\nu-1}\nu^{a-1} e^{-b\nu}}{\left(\Gamma(\nu) \right)^n}$$

which is not a recognizable distribution. We use a Metropolis within Gibbs algorithm to sample from the full conditionals, using a random walk proposal on $log(\nu)$. I tried various hyperparameters appropriate for this data.

```{r, echo=F, include=F}
x = read.table("my-data.txt", header = F)[,1]
n = length(x)
sum_x = sum(x)
sum_logx = sum(log(x))
library(coda)
library(mvtnorm)
library(MASS)

nu_condit = function(nu, theta_curr){
  return(nu*(sum_logx + n*log(theta_curr)-1)-n*lgamma(nu)+3*log(nu))
}
pcurr = function(nu_curr, theta_curr){
  return((n+nu_curr+2)*log(theta_curr)-n*lgamma(nu_curr) + nu_curr*(sum_logx-1) + 3*log(nu_curr) - theta_curr*(2+sum_x))
}
N = 1000
```

```{r}
sample = NULL
sample$theta = rep(NA,N)
sample$nu = rep(NA,N)
alpha = 3
beta = 2
v = 0.05
theta_curr = 2
nu_curr = 3
set.seed(2)
for(i in 1:N){
  theta_curr = rgamma(1, n*nu_curr + alpha, beta + sum_x)
  nu_new = exp(log(nu_curr) + rnorm(1,0,sqrt(v)))
  pnu_curr = nu_condit(nu_curr, theta_curr)
  pnu_new = nu_condit(nu_new, theta_curr)
  accept = exp(pnu_new - pnu_curr)
  if(runif(1) < accept)
    nu_curr = nu_new
  
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_new
}
```


The effective sample size for $\theta$ is
```{r, echo=F}
effectiveSize(sample$theta)
```

The effective sample size for $\nu$ is 
```{r, echo = F}
effectiveSize(sample$nu)
```

The table below summarizes the results

\begin{center}
\begin{tabular}{ |c|c|c| }
\hline
 parameter & mean & 95\% Credible Interval \\ 
 \hline
 $\theta$ & 1.13 & (0.635,1.846) \\  
 $\nu$ & 2.807 & (1.669, 4.55)\\
 \hline
\end{tabular}
\end{center}

The traceplots are below

```{r, echo=F}
par(mfrow = c(1,2))
plot.ts(sample$theta, ylab = expression(theta), xlab = "", main = "Trace")
plot.ts(sample$nu, ylab = expression(nu), xlab = "", main = "Trace")
```

The autocorrelation plots are below

```{r, echo=F}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2.1, 2.1))
autocorr.plot(sample$theta, col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(theta)) 
autocorr.plot(sample$nu, col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(nu))
```

## 2B

Now we develop a Metropolis-Hastings algorithm that jointly proposes $\log(\nu)$ and $\log(\theta)$ using
a Gaussian random walk centered on the current value of the parameters. Tune the
variance-covariance matrix of the proposal using a test run that proposes the parameters
independently:
```{r,echo=F}
N_test = 5000
```

```{r}
V = 0.05*diag(2)
theta_curr = 2
nu_curr = 3

for(i in 1:N_test){
  nu_new = exp(log(nu_curr) + rnorm(1,0,sqrt(V[1,1])))
  theta_new = exp(log(theta_curr) + rnorm(1,0, V[2,2]))
  p_curr = pcurr(nu_curr, theta_curr)
  p_new = pcurr(nu_new, theta_new)
  accept = exp(p_new - p_curr)
  if(runif(1) < accept){
    nu_curr = nu_new
    theta_curr = theta_new
  }
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_curr
}
```


```{r,echo=F}
V = cov(cbind(sample$nu, sample$theta))


pcurr2 = function(nu,theta){
  return(n*nu*log(theta) - n*lgamma(nu) + nu *(sum_logx -1) + 3*log(nu) + log(theta) - theta * (2 + sum_x))
}
N=10000
```

```{r}
for(i in N_test+1:N){
  new = mvrnorm(1, c(log(nu_curr), log(theta_curr)), V)
  nu_new = exp(new[1])
  theta_new = exp(new[2])
  p_curr = pcurr2(nu_curr, theta_curr)
  p_new = pcurr2(nu_new, theta_new)
  acceptance = exp(p_new - p_curr)
  if(runif(1) < acceptance){
    nu_curr = nu_new
    theta_curr = theta_new
  }
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_curr
}
```


\begin{center}
\begin{tabular}{ |c|c|c| }
\hline
 parameter & mean & 95\% Credible Interval \\ 
 \hline
 $\theta$ & 1.11 & (0.59,1.79) \\  
 $\nu$ & 2.807 & (1.49, 4.38)\\
 \hline
\end{tabular}
\end{center}

The trace plot for these samples are below

```{r, echo=F}
par(mfrow = c(1,2))
plot.ts(tail(sample$theta,5000),ylab = expression(theta), xlab = "", main = "Trace")
plot.ts(tail(sample$nu,5000), ylab = expression(nu), xlab = "", main = "Trace")
```

The corresponding autocorrelation plot is below

```{r,echo=F}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2.1, 2.1))
autocorr.plot(tail(sample$theta,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(theta)) 
autocorr.plot(tail(sample$nu,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(nu))
```

## 2C

Now we are going to develop a Metropolis algorithm that jointly proposes $\log\nu$ and $\log\theta$ using independent proposals based on Laplace approximation of the posterior distribution of $\log\nu$ and $\log\theta$. 

We let $t=\log\theta$ and $v =\log\nu$, then the posterior becomes 
\begin{align*} 
\pi(\theta,\nu|\pmb{x}) &\propto \exp\left\{(\nu -1) \sum_{i=1}^n \log x_i +(a -1)\log\nu - b\nu -n\log\Gamma(\nu) \right\}\\
&\times \exp\left\{(\alpha +n\nu-1)\log\theta -\theta\left(\beta + \sum_{i=1}^n x_i \right)\right\}\\
\Rightarrow \pi(t,v|\pmb{x}) &\propto \exp\left\{(e^v -1) sum_{i=1}^n \log x_i + av - be^v -n\log \Gamma(e^v)\right\}\\
&\times \exp\left\{(a +ne^v)t - e^t\left(\beta + sum_{i=1}^n x_i\right) \right\}
\end{align*}

Now, we let
$$h(t,v) = (e^v =1)\sum_{i=1}^n x_i + av -be^v-n\log \Gamma(e^v)\exp\left\{(a +ne^v)t - e^t\left(\beta + sum_{i=1}^n x_i\right) \right\}$$

Then we use the definition of Laplace approximation

```{r}
h = function(w) {
  a1 = (exp(w[2]) - 1) * sum_logx + 3 * w[2] - exp(w[2])
  return(-(a1 - n * lgamma(exp(w[2])) + 
             (2 + n * exp(w[2])) * w[1] - exp(w[1]) * (2 + sum_x)))
}
laplace = optim(c(0,1), h, hessian = T)
```

The laplace maximum for the parameters are
```{r,echo=F}
laplace$par
```
and the hessian obtained at the maximum is

```{r, echo=F}
laplace$hessian
V = diag(diag(solve(laplace$hessian)))
```

Now we update the variance-covariance matrix then resume the Metropolis sampling algorithm
```{r}
for(i in N_test+1:N){
  nu_new = exp(log(nu_curr) + rnorm(1,0,sqrt(V[1,1])))
  theta_new = exp(log(theta_curr) + rnorm(1,0,sqrt(V[2,2])))
  p_curr = pcurr(nu_curr = nu_curr, theta_curr = theta_curr)
  p_new =pcurr(nu_curr = nu_new, theta_curr = theta_new)
  
  accept = exp(p_new - p_curr)
  if(runif(1) < accept){
    nu_curr = nu_new
    theta_curr = theta_new
  }
  sample$theta[i] = theta_curr
  sample$nu[i] = nu_curr
}
```

The corresponding traceplots are below

```{r, echo =F}
par(mfrow = c(1,2))
plot.ts(tail(sample$theta,5000), xlab="", ylab = expression(theta), main = "Traceplot")
plot.ts(tail(sample$nu,5000), xlab="", ylab = expression(nu), main = "Traceplot")
```

The corresponding autocorrelation plots are below

```{r,echo=F}
par(mfrow=c(1,2), mar=c(4.5, 4.5, 2.1, 2.1))
autocorr.plot(tail(sample$theta,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(theta)) 
autocorr.plot(tail(sample$nu,5000), col=1, lwd=4, cex.axis=1.5, cex.lab=1.5, auto.layout = FALSE, main=expression(nu))
```

The effective sample size associated with $\theta$ is
```{r,echo=F}
effectiveSize(tail(sample$theta,5000))
```

The effective sample size associated with $\nu$ is
```{r,echo=F}
effectiveSize(tail(sample$nu,5000))
```

\begin{center}
\begin{tabular}{ |c|c|c| }
\hline
 parameter & mean & 95\% Credible Interval \\ 
 \hline
 $\theta$ & 1.05 & (0.56,1.58) \\  
 $\nu$ & 2.55 & (1.49, 3.79)\\
 \hline
\end{tabular}
\end{center}

## 3

Given the random effects model we have $(y_{ij} -(\beta + u_i)) \sim N(0,\tau^2)$, $u_i \sim N(0,\tau^2)$, and $\pi(\beta,\sigma^2,\tau^2) \propto (\sigma^2 \tau^2)^{-1}$. Then the joint posterior is

$$\pi(u_i,\beta,\tau^2,\sigma^2|y) \propto (\tau^2)^{-\left(\frac{IJ}{2}+1\right)} (\sigma^2)^{-\left(\frac{I}{2}+1\right)}\exp\left\{-\frac{1}{2\tau^2} \sum_{ij} \left(y_{ij} - (\beta + u_i)\right)^2  - \frac{1}{2\tau^2} \sum u_i^2 \right\} $$

### 3A

i) 

$$\pi(u_i|y,\beta,\tau,\sigma^2) \propto \exp \left\{-\frac{1}{2\tau^2} \sum \left[y^2_{ij} - 2y_{ij}(\beta + u_i) + (\beta + u_i)^2 \right] - \frac{1}{2\sigma^2} \sum u_i^2 \right\} $$
$$= \exp\left\{-\frac{1}{2\tau^2}\left[\sum (-2y_{ij}u_i) + \sum (2\beta u_i + u_i^2) \right] - \frac{1}{2\sigma^2}\sum u_i^2\right\} $$
$$= \exp\left\{-\frac{1}{2\tau^2}\left[\sum u_i^2 - 2 \sum u_i(y_{ij} - \beta) \right] - \frac{1}{2\sigma^2} \sum u_i^2 \right\} $$
$$= \exp\left\{-\frac{1}{2\tau^2} \left[\sum Ju_i^2 - 2 \sum u_i(y_{ij} -\beta) \right] - \frac{1}{2\sigma^2} u_i^2 \right\} $$
$$ = \exp \left\{-\frac{1}{2\tau^2\sigma^2} \left[J\sigma^2 \sum u_i^2 - 2\sigma^2 \sum u_i(y_{ij} - \beta) + \tau^2 \sum u_i^2 \right] \right\}$$
$$ = \exp\left\{-\frac{1}{2\tau^2\sigma^2} \left(u_i^2 \left(J\sigma^2 + \tau^2 \right) -2 u_i \sum (y_{ij} -\beta) \right) \right\} $$
$$ = \exp\left\{-\frac{J\sigma^2 +\tau^2}{2\sigma^2\tau^2 }\left(u_i^2 - 2u_i \frac{\sum (y_{ij} -\beta)}{J\sigma^2 + \tau^2} \right) \right\} $$
Therefore, 
$$ u_i | \cdot \sim N\left(\frac{\sum_j (y_{ij} -\beta)}{J\sigma^2 + \tau^2}, \frac{\tau^2\sigma^2}{J\sigma^2 + \tau^2} \right) = N\left(\left(\frac{J}{\tau^2} + \frac{1}{\sigma^2} \right)^{-1} \left(\frac{\sum_j (y_{ij} - \beta)}{\tau^2} \right), \left(\frac{J}{\tau^2} + \frac{1}{\sigma^2} \right)^{-1} \right)$$

I am lazy, so I am just going to skip to the end results so I don't have to type all my work :(

ii) 
$$\beta|\cdot \sim N\left(\frac{\tau^2}{IJ}, \frac{\sum_{ij} (y_{ij} - u_i)}{IJ} \right) = N\left(\left(\frac{IJ}{\tau^2} \right)^{-1} \left(\frac{\sum_{ij} (y_{ij} - u_i)}{\tau^2}\right), \left(\frac{IJ}{\tau^2} \right)^{-1} \right) $$

iii)
$$\sigma^2|\cdot \sim IG\left(\frac{I}{2}, \frac{1}{2} \sum_i u_i^2 \right) $$

iv)
$$ \tau^2|\cdot \sim IG \left( \frac{IJ}{2}, \frac{1}{2} \sum_{ij} (y_{ij} - (\beta + u_i))^2 \right) $$

## 3B
$$\pi(\beta, \tau^2, \sigma^2|y) \propto (\tau^2)^{-\left(\frac{I(J-1)}{2} + 1 \right)} (\sigma^2)^{-1} (J\sigma^2 + \tau^2) ^{I/2} \exp\left\{ -\frac{1}{2\tau^2} \sum_{ij} (y_{ij} - \beta)^2 \right\}$$



$$\times \exp\left\{\frac{\sigma^2}{2\tau^2(J\sigma^2 + \tau^2)} \sum_i \left(\sum_j (y_{ij}-\beta) \right)^2 \right\} $$

## 3C

$$\pi(\tau^2, \sigma^2|y) \propto (\tau^2)^{-\left(\frac{I(J-1)}{2} + 1 \right)} (\sigma^2)^{-1} \left(J\sigma^2 + \tau^2 \right) ^{\frac{I+1}{2}} \exp \left\{-\frac{1}{2\tau^2} \sum_{ij} y_{ij}^2 \right\}$$
$$ \times \exp\left\{ \frac{\sigma^2}{2\tau^2 (J\sigma^2 + \tau^2)} \sum_i \left( \sum_j y_{ij}^2 \right)\right\}$$
$$ \times \exp\left\{\frac{1}{2IJ (J\sigma^2 + \tau^2)} \left(\sum_{ij} y_{ij} \right)^2 \right\} $$

## 4

The joint posterior is obtained by 

\begin{align*}
    \pi(\theta,\phi,m|\pmb{y}) \propto & f(\pmb{y}| \theta, \phi, m) \pi(\theta)\pi(\phi)\pi(m)\\
    \propto & \theta^{\sum_{i=1}^m y_i + \alpha -1} \exp \left\{-\theta\left(\beta + m \right) \right\} \phi^{\sum_{i = m+1}^n y_i + \gamma -1} \exp\left\{-\phi (\delta +n -m) \right\}
\end{align*}


Then the full conditionals are as follows
$$\phi|m,\pmb{y} \sim Gamma\left(\sum_{i = m+1}^n y_i + \gamma -1, \delta + n -m\right) $$
$$\theta|m,\pmb{y} \sim Gamma \left(\sum_{i=1}^m y_i + \alpha -1, \beta +m \right) $$
$$\pi(m|\theta,\phi,\pmb{y}) \propto \theta^{\sum_{i=1}^m y_i +\alpha -1} \exp\left\{-\theta(\beta +m) \right\} \phi^{\sum_{i=m+1}^n y_i +\gamma -1} \exp\left\{-\phi(\delta +n -m) \right\} $$
We now use a Metropolis-within-Gibbs method because it converges better.

```{r, echo=F}
# upload mining data
y <-c(4,5,4,1,0,4,3,4,0,6,3,3,4,0,2,6,3,3,5,4,5,3,1,4,4,1,5,5,3,4,2,5,2,2,3,4,2,1,3,2,2,1, 1,1,1,3,0,0,1,0,1,1,0,0,3,1,0,3,2,2,0,1,1,1,0,1,0,1,0,0,0,2,1,0,0,0,1,1,0,2,3,3,1,1,2,1,1, 1,1,2,4,2,0,0,0,1,4,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1)
n <- length(y)
# specify hyperparameters. In this case using the data.
alpha <- 3
beta <- alpha/mean(y[1:40]) 
gam <- 3
delta <- gam/mean(y[-(1:40)])

# set up MCMC variables
N <- 50000
N.burn <- 5000
sample_save <- NULL
sample_save$m <- rep(NA, N) 
sample_save$th <- rep(NA, N) 
sample_save$phi <- rep(NA, N)

# initialize chains
theta_curr <- rgamma(1, alpha, beta) 
phi_curr <- rgamma(1, gam, delta) 
m_curr <- 40
set.seed(2)
```

```{r}
# Algorithm
for(i in 1:N){
  theta_curr <- rgamma(1, sum(y[1:m_curr]) + alpha, m_curr + beta)
  
  phi_curr <- rgamma(1, sum(y[-(1:m_curr)]) + gam, (n-m_curr + delta))
  
  m_new <- sample((1:n), 1, FALSE)
  
  p_curr <- lgamma(sum(y[1:m_curr]) + alpha) - (sum(y[1:m_curr]) + alpha)*log(m_curr + beta) +lgamma(sum(y[-(1:m_curr)]) + gam) - (sum(y[-(1:m_curr)]) + gam)*log(n-m_curr + delta)
  
  p_new <- lgamma(sum(y[1:m_new]) + alpha) - (sum(y[1:m_new]) + alpha)*log(m_new + beta) +lgamma(sum(y[-(1:m_new)]) + gam) - (sum(y[-(1:m_new)]) + gam)*log(n-m_new + delta)
  # calculate acceptance probability and accept/reject acordingly
  accpt.prob <- exp(p_new - p_curr) 
  if(runif(1) < accpt.prob)
  {
    m_curr <- m_new
  }
  # save the current draws
  sample_save$theta[i] <- theta_curr
  sample_save$phi[i] <- phi_curr
  sample_save$m[i] <- m_curr
}
```


Using $N=50000$ samples and a burnin of 5000, the results are plotted below with the red line signifying the mean for the parameters

```{r, echo=F}
par(mfrow = c(1,2))
plot.ts(tail(sample_save$phi,5000), main = "Traceplot", ylab = expression(phi), xlab = "")
abline(h = mean(tail(sample_save$phi,5000)), col = "red")

hist(tail(sample_save$phi,5000), xlab = expression(phi), main = expression("Histogram for " ~ phi))
abline(v = mean(tail(sample_save$phi,5000)), col = "red")

```

```{r, echo=F}
par(mfrow = c(1,2))
plot.ts(tail(sample_save$theta,5000), main = "Traceplot", ylab = expression(theta), xlab = "")
abline(h = mean(tail(sample_save$theta,5000)), col = "red")
hist(tail(sample_save$theta,5000), xlab = expression(theta), main = expression("Histogram for " ~ theta))
abline(v = mean(tail(sample_save$theta,5000)), col = "red")
```

```{r, echo=F}
hist(tail(sample_save$m,5000), xlab = expression(m), main = expression("Histogram for " ~ m))
abline(v = mean(tail(sample_save$m,5000)), col = "red")
```


## 5 
Given the following
$$y_{ij} \sim N\left(\alpha_i + \beta_i t_{ij}, \sigma^2 \right) $$
$$ (\alpha_i, \beta_i)'|\alpha, \beta \sim N\left((\alpha,\beta)', diag(\tau_\alpha^{-1}, \tau_\beta^{-1}) \right) $$
$$(\alpha, \beta)' \sim N \left((0,0)', diag(P_\alpha^{-1}, P_\beta^{-1}) \right)  $$
Then we obtain the following joint posterior
$$\pi(\cdot|\pmb{y}) \propto (\sigma^2)^{n/2} \exp\left\{-\frac{1}{2\sigma^2} \sum_{ij}(y_{ij} - (\alpha_i + \beta_i t_{ij}))^2 \right\} $$
$$\times (\tau_\alpha)^{I/2}(\tau_\beta)^{I/2}\exp\left\{-\frac{1}{2}\left[P_\alpha \alpha^2 + P_\beta  \beta^2\right] \right\} $$
$$\times \exp\left\{- \frac{1}{2}\left[\tau_\alpha \sum_{i = 1}^I (\alpha - \alpha_i)^2 + \tau_\beta \sum_{i = 1}^I(\beta - \beta_i)^2 \right] \right\} $$
$$\times (\sigma^{-2}\tau_\alpha \tau_\beta)^{a-1} \exp \left\{ -b(\sigma^{-2} + \tau_\alpha + \tau_\beta) \right\} $$
Which lead to the following full conditionals

$$(\alpha|\pmb{y}, \cdot) \sim N \left( \frac{\tau_\alpha \sum_{i=1}^I \alpha_i}{ I\tau_\alpha + P_\alpha }, \frac{1}{I\tau_\alpha + P_\alpha} \right) $$
$$(\beta| \pmb{y}, \cdot) \sim N \left(\frac{\tau_\beta \sum_{i=1}^I \beta_i}{ I\tau_\beta + P_\beta}, \frac{1}{I\tau_\beta + P_\beta} \right) $$

$$(\tau_\alpha|\pmb{y}, \cdot) \sim Gamma\left(a + \frac{I}{2}, b + \frac{1}{2}  \sum_{i=1}^I (\alpha - \alpha_i)^2 \right) $$
$$ (\tau_\beta|\pmb{y}, \cdot) \sim Gamma \left(a + \frac{I}{2}, \frac{1}{2} \sum_{i=1}^I (\beta-\beta_i)^2 \right) $$
$$(\sigma^2|\pmb{y}, \cdot) \sim Gamma \left(a + \frac{n}{2}, b + \frac{1}{2}  \sum_{j=1}^{n_i} (y_{ij} - (\alpha_i + \beta_i t_{ij} ))^2 \right) $$
$$(\alpha_i| \pmb{y}, \cdot) \sim N \left(\frac{\alpha \tau_\alpha + \sigma^{-2} \sum_{j=1}^{n_i} (y_{ij} - \beta_i t_{ij}) }{\tau_\alpha + \sigma^{-2} \sum_{j=1}^{n_i}t_{ij} }, \frac{1}{\tau_\alpha + \sigma^{-2} \sum_{j=1}^{n_i}t_{ij}} \right)$$
$$(\beta_i|\pmb{y},\cdot) \sim N\left( \frac{\beta \tau_\beta + \sigma^{-2}\sum_{j=1}^{n_i}(y_{ij} - \alpha_i ) }{\tau_\beta + \sigma^{-2} \sum_{j=1}^{n_i} t_{ij}^2 }, \frac{1}{\tau_\beta + \sigma^{-2} \sum_{j=1}^{n_i} t_{ij}^2} \right) $$

In the above the $\cdot$ represents all of the parameters besides the one being conditioned on.